---
title: "Classification Models For Legendary Pokemon"
author: "Alex Ryan"
date: "May 9, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This is a project that focuses on classifying Non-Legendary Pokemon from Legendary Pokemon. I will explore the data with visualizations and explanations. I will also utilize knn, naive bayes, and xgbtree. These models will attempt to distinguish between these two types of pokemon.


## Data Dictionary

The Pokemon dataset was taken from Kaggle. The pokemon included come from Generations 1-6. The dataset includes attributes such as id number, name, Type.1, Type.2, total, HP, Attack, Defense, Sp.Atk, Sp.Def, Speed, Generation, and Legendary. Added features will be discussed in later sections.

Variable Name:  | Variable Description:
---------------| ---------------------------------------------------------------------------
'id'           | Identification number of pokemon
'name'        | Name of Pokemon
'Type.1'  | Primary property trait of pokemon
'Type.2' | Secondary property trait of pokemon
'total' | Total stats of pokemon
'HP' | Total Health of pokemon
'Attack' | Total Attack of pokemon
'Defense' | Total Defense of pokemon
'Sp.Atk' | Total Special Attack of pokemon
'Sp.Def' | Total Special Defense of pokemon
'Speed' | Total Speed of pokemon
'Generation' | The generation number of pokemon
'Legendary' | A true or false value that defines whether the pokemon is a legendary or not

## Data Loading and Packages Used
So in order to create to the visualizations and use knn, naive bayes, and xgbtrees, I loaded in all of the packages below as well as the ggplot2 and the xgboost package. I also loaded in my data.

```{r message=FALSE}
library(caret)
library(doSNOW)
library(magrittr)
library(dplyr)
library(ggthemes)
library(fmsb)
library(e1071)
pokemon = read.csv("Pokemon.csv", stringsAsFactors = FALSE)
```

## Exploratory Analysis with Visualizations
```{r echo = FALSE, eval = TRUE, fig.width=4, fig.height=4, fig.align='center'}
colnames(pokemon)<-c("id","Name","Type.1","Type.2", "Total", "HP","Attack","Defense","Sp.Atk","Sp.Def","Speed","Generation","Legendary")
Type.1<-c("Dragon","Steel","Flying","Psychic","Rock" ,"Fire","Electric" ,"Dark","Ghost" ,"Ground","Ice", "Water","Grass","Fighting", "Fairy" ,"Poison","Normal","Bug")
color<-c("#6F35FC","#B7B7CE","#A98FF3","#F95587","#B6A136","#EE8130","#F7D02C","#705746","#735797","#E2BF65","#96D9D6","#6390F0","#7AC74C","#C22E28","#D685AD","#A33EA1","#A8A77A","#A6B91A")
COL<-data.frame(Type.1,color)

merge(
  merge(pokemon %>% dplyr::group_by(Type.1) %>% dplyr::summarize(tot=n()),
        pokemon %>% dplyr::group_by(Type.1,Legendary) %>% dplyr::summarize(count=n()),by='Type.1'),
  COL, by='Type.1') %>% 
  ggplot(aes(x=reorder(Type.1,tot),y=count)) + 
  geom_bar(aes(fill=color,alpha=Legendary),color='white',size=.25,stat='identity') + 
  scale_fill_identity() + coord_flip() + theme_fivethirtyeight() + 
  ggtitle("Pokemon Distribution") + scale_alpha_discrete(range=c(.9,.6))
```

This graph displays the number of pokemon for each primary type and proportion of legendaries in each type. The Psychic, Dragon, and Flying Types have a higher proportion of Legendaries than the other types. So I decided to create three features that target those three types of pokemon. There is also another feature I created based on the graph below.

```{r echo = FALSE, eval = TRUE, fig.width=12, fig.height=12, fig.align='center'}
res<-data.frame(pokemon %>% dplyr::select(Type.1,HP, Attack, Defense, Sp.Atk, Sp.Def, Speed) %>% dplyr::group_by(Type.1) %>% dplyr::summarise_all(funs(mean)) %>% mutate(sumChars = HP + Attack + Defense + Sp.Atk + Sp.Def + Speed) %>% arrange(-sumChars))
res$color<-color
max<- ceiling(apply(res[,2:7], 2, function(x) max(x, na.rm = TRUE)) %>% sapply(as.double)) %>% as.vector
min<-rep.int(0,6)
par(mfrow=c(3,6))
par(mar=c(1,1,1,1))
for(i in 1:nrow(res)){
  curCol<-(col2rgb(as.character(res$color[i]))%>% as.integer())/255
  radarchart(rbind(max,min,res[i,2:7]),
             axistype=2 , 
             pcol=rgb(curCol[1],curCol[2],curCol[3], alpha = 1) ,
             pfcol=rgb(curCol[1],curCol[2],curCol[3],.5) ,
             plwd=2 , cglcol="grey", cglty=1, 
             axislabcol="black", caxislabels=seq(0,2000,5), cglwd=0.8, vlcex=0.8,
             title=as.character(res$Type.1[i]))
}
```

In this graph, we can see the average base stat distribution in comparision to the primary type's average base stat distribution. The three categories of Psychic, Dragon, and Flying all have at least average to above average stat distribution for each stat. Because each of those three primary types have a high proportion of legendaries, I decided to create a total stat strength feature to help improve legendary classification. 

## Features Created

This is the feature code for checking if the pokemon is a dragon, psychic, or flying. These three types all have a high proportion of legendary pokemon.

```{r message=FALSE}
#Features to distinguish pokemon of these three types
pokemon$isDragon <- ifelse(pokemon$Type.1 == "Dragon", "Y", "N")
pokemon$isPsychic <- ifelse(pokemon$Type.1 == "Psychic", "Y", "N")
pokemon$isFlying <- ifelse(pokemon$Type.1 == "Flying", "Y", "N")
```

This is the feature code for checking if pokemon has higher total base stats than the median pokemon. This is because legendaries and the three high proportion types have higher base stats than the average pokemon.

```{r message=FALSE}
#Features to 
pokemon$OverallStrength <- ifelse(pokemon$Total > median(pokemon$Total),
                                "Strong", "Weak")
```

## Factors Used for Classification
In order to have our classification recognize our categorical data, we need to create factors for each column that we are using. This includes the features we created above.

```{r message=FALSE}
#Factoring Categorical Data and Features
pokemon$Type.1 <- as.factor(pokemon$Type.1)
pokemon$Type.2 <- as.factor(pokemon$Type.2)
pokemon$Generation <- as.factor(pokemon$Generation)
pokemon$Legendary <- as.factor(pokemon$Legendary)
pokemon$OverallStrength <- as.factor(pokemon$OverallStrength)
pokemon$isDragon <- as.factor(pokemon$isDragon)
pokemon$isPsychic <- as.factor(pokemon$isPsychic)
pokemon$isFlying <- as.factor(pokemon$isFlying)
features <- c("Type.1", "Type.2", "Generation", "Legendary","OverallStrength", 
              "isDragon", "isPsychic", "isFlying")
pokemon <- pokemon[, features]
```

## Train and Test Partitions
The train and test splits were partitioned in a proportion of 80% of the data going to train and 20% of the data going to test. Both of the splits were roughly in same proportion of legendary and non-legendary like the original dataset.

```{r message=FALSE}
# Creating Train and Test Splits
set.seed(54234)
indexes <- createDataPartition(pokemon$Legendary,
                               times = 1,
                               p = 0.8,
                               list = FALSE)
pokemon.train <- pokemon[indexes,]
pokemon.test <- pokemon[-indexes,]

# Examine the proportions of the Survived class lable across
# the datasets.
prop.table(table(pokemon$Legendary))
prop.table(table(pokemon.train$Legendary))
prop.table(table(pokemon.test$Legendary))

```

## Cross Validation and Hypertuning For XGBoost
Creating a process for cross validation to occur 10 times and be repeated 3 times, making a grand total of 30 times. I also used a tuning grid for the xgboost in order to optimize the tuning parameters.

```{r message=FALSE}
# Cross validation and tuning for model
train.control <- trainControl(method = "repeatedcv",
                              number = 10,
                              repeats = 3,
                              search = "grid")

# Tuning grid for xgboost trees
tune.grid <- expand.grid(eta = c( 0.1, 0.2, 0.3, 0.4, 0.5),
                         nrounds = c(50, 75, 100),
                         max_depth = 4:8,
                         min_child_weight = c(2.0, 2.25, 2.5),
                         colsample_bytree = c(0.5, 0.75, 1.0),
                         gamma = 0,
                         subsample = 1)
```

## Creating a Cluster to Obtain Optimatal Parameters for XGBoost
I utilized doSNOW to create a cluster with 3 cores. I recommend only running 2 or less cores, if you do not have a workstation quality computer.I then inputted the parameters into the prediction and displayed the results in a confusion matrix, which compares the predictions to the actual results.

```{r message=FALSE}
#Creating Cluster
cl <- makeCluster(3, type = "SOCK")
registerDoSNOW(cl)

#Gather training model with 3 clusters to identify legendary pokemon
caret.cv <- train(Legendary ~ ., 
                  data = pokemon.train,
                  method = "xgbTree",
                  tuneGrid = tune.grid,
                  trControl = train.control)
stopCluster(cl)

#Predict and output results in a confusion matrix
xgbHat = predict(caret.cv, pokemon.test)
confusionMatrix(xgbHat, pokemon.test$Legendary)
```

## Naive Bayesian Classifier 
I also created a Naive Bayesian Classifier model with a Laplace value of 1. The confusion matrix results are outputted below.

```{r message=FALSE}
NB = naiveBayes(pokemon.train, pokemon.train$Legendary,laplace = 1)
nbHat = predict(NB,pokemon.test$Legendary)
confusionMatrix(nbHat, pokemon.test$Legendary)
```

## KNN Classifier
The KNN classifier I created was cross validated with an optimal k value. The confusion matrix results are outputted below.

```{r message=FALSE}
#KNN Classification
cl <- makeCluster(3, type = "SOCK")
registerDoSNOW(cl)

knn_fit <- train(Legendary ~., data = pokemon.train, method = "knn",
                 trControl= train.control,
                 preProcess = c("center", "scale"),
                 tuneLength = 10)

stopCluster(cl)

knnHat = predict(knn_fit, pokemon.test)
confusionMatrix(knnHat, pokemon.test$Legendary)
```