smsFreqTrain = smsTrain[ , smsFreqWords]
smsFreqTest = smsTest[ , smsFreqWords]
# counts -> binary
smsTrain = apply(smsFreqTrain, MARGIN = 2, convertCounts)
smsTest  = apply(smsFreqTest, MARGIN = 2, convertCounts)
smsNB = naiveBayes(smsTrain, smsTrainy, laplace=i)
#pred and misclass
yhat = predict(smsNB,smsTest)
ctab = table(yhat,smsTesty)
ctab
misclass = (sum(ctab)-sum(diag(ctab)))/sum(ctab)
perspam = ctab[2,2]/sum(ctab[,2])
cat("Laplace: ", i, "misclass,perspam: ", misclass,perspam,"\n")
}
knitr::opts_chunk$set(dev = 'pdf')
n=10;p=2
set.seed(14)
x = matrix(rnorm(n*p),ncol=p)
x
trainfrac=.75; nTrain = floor(n*trainfrac)
set.seed(99)
ii = sample(1:n,nTrain)
print(ii)
xtrain = x[ii,]
xtest = x[-ii,]
print(xtrain)
print(xtest)
# read in data
smsRaw = read.csv("http://www.rob-mcculloch.org/data/sms_spam.csv", stringsAsFactors = FALSE)
# convert spam/ham to factor.
smsRaw$type = factor(smsRaw$type)
#look at y=type
print(table(smsRaw$type))
#look at x=words
library(wordcloud)
wordcloud(smsRaw$text, max.words = 40)
# build a corpus using the text mining (tm) package
library(tm)
library(SnowballC)
#volatile (in memory corpus from vector of text in R
smsC = VCorpus(VectorSource(smsRaw$text))
# clean up the corpus using tm_map()
smsCC = tm_map(smsC, content_transformer(tolower)) #upper -> lower
smsCC = tm_map(smsCC, removeNumbers) # remove numbers
smsCC = tm_map(smsCC, removeWords, stopwords()) # remove stop words
smsCC = tm_map(smsCC, removePunctuation) # remove punctuation
smsCC = tm_map(smsCC, stemDocument) #stemming
smsCC = tm_map(smsCC, stripWhitespace) # eliminate unneeded whitespace
# create Document Term Matrix
smsDtm <- DocumentTermMatrix(smsC, control = list(
tolower = TRUE,
removeNumbers = TRUE,
stopwords = TRUE,
removePunctuation = TRUE,
stemming = TRUE
))
dim(smsDtm)
# creating training and test datasets
smsTrain = smsDtm[1:4169, ]
smsTest  = smsDtm[4170:5559, ]
smsTrainy = smsRaw[1:4169, ]$type
smsTesty  = smsRaw[4170:5559, ]$type
cat("training fraction is: ",4169/5559,"\n")
smsFreqWords = findFreqTerms(smsTrain, 5) #words that appear at leat 5 times
smsFreqTrain = smsTrain[ , smsFreqWords]
smsFreqTest = smsTest[ , smsFreqWords]
convertCounts <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}
# apply() convert_counts() to columns of train/test data
smsTrain = apply(smsFreqTrain, MARGIN = 2, convertCounts)
smsTest  = apply(smsFreqTest, MARGIN = 2, convertCounts)
library(e1071)
smsNB = naiveBayes(smsTrain, smsTrainy, laplace=1)
yhat = predict(smsNB,smsTest)
ctab = table(yhat,smsTesty)
ctab
misclass = (sum(ctab)-sum(diag(ctab)))/sum(ctab)
perspam = ctab[2,2]/sum(ctab[,2])
cat("misclass,perspam: ", misclass,perspam,"\n")
# sample train/test
for ( i in 1:20)
{
trainfrac=.75
n= length(smsRaw$type)
nTrain = floor(trainfrac*n)
set.seed(99)
ii = sample(1:n,nTrain)
smsTrain = smsDtm[ii, ]
smsTest  = smsDtm[-ii, ]
smsTrainy = smsRaw[ii, ]$type
smsTesty  = smsRaw[-ii, ]$type
# freq words
smsFreqWords = findFreqTerms(smsTrain, 5) #words that appear at leat 5 times
smsFreqTrain = smsTrain[ , smsFreqWords]
smsFreqTest = smsTest[ , smsFreqWords]
# counts -> binary
smsTrain = apply(smsFreqTrain, MARGIN = 2, convertCounts)
smsTest  = apply(smsFreqTest, MARGIN = 2, convertCounts)
smsNB = naiveBayes(smsTrain, smsTrainy, laplace=i)
#pred and misclass
yhat = predict(smsNB,smsTest)
ctab = table(yhat,smsTesty)
ctab
misclass = (sum(ctab)-sum(diag(ctab)))/sum(ctab)
perspam = ctab[2,2]/sum(ctab[,2])
cat("Laplace: ", i, "misclass,perspam: ", misclass,perspam,"\n")
}
install.packages("kknn")
smsRaw = read.csv("http://www.rob-mcculloch.org/data/sms_spam.csv", stringsAsFactors = FALSE)
library(kknn)
smsRaw = read.csv("http://www.rob-mcculloch.org/data/sms_spam.csv", stringsAsFactors = FALSE)
source('~/.active-rstudio-document')
source('~/.active-rstudio-document', echo=TRUE)
library(kknn)
smsRaw = read.csv("http://www.rob-mcculloch.org/data/sms_spam.csv", stringsAsFactors = FALSE)
library(kknn)
smsRaw = read.csv("http://www.rob-mcculloch.org/data/susedcars.csv", stringsAsFactors = FALSE)
dim(smsRaw)
partOne = smsRaw[1:4,]
partOne = smsRaw
summary(partOne)
ls()
attach(partOne)
library(kknn)
smsRaw = read.csv("http://www.rob-mcculloch.org/data/susedcars.csv", stringsAsFactors = FALSE)
dim(smsRaw)
partOne = smsRaw
summary(partOne)
ls()
attach(partOne)
ls(pos = 1)
library(kknn)
smsRaw = read.csv("http://www.rob-mcculloch.org/data/susedcars.csv", stringsAsFactors = FALSE)
dim(smsRaw)
ls()
attach(smsRaw)
sd = read.csv("http://www.rob-mcculloch.org/data/Ayx.csv")
facv = c(1,2,3,5,6)
for(i in facv) sd[[i]]=as.factor(sd[[i]]) #convert categoricals to factors
summary(sd)
dim(sd)
library(nnet)
mfit = multinom(yr~gender+age,data=sd)
summary(mfit)
yhat = predict(mfit,sd) #most likely
phat = predict(mfit,sd,type="probs") # probs
yhat[1:5]
phat[1:5,]
table(yhat,sd$yr)
table(yhat,sd$yr~gender)
table(yhat,sd$yr,sd$age)
table(yhat,sd$yr)
mfit = multinom(yr~gender+age+yc+empstat+yrres+numdep,data=sd)
summary(mfit)
yhat = predict(mfit,sd) #most likely for model for all of x variables
table(yhat,sd$yr)
mfit = multinom(yr~gender+age,data=sd)
summary(mfit)
yhat = predict(mfit,sd) #most likely
phat = predict(mfit,sd,type="probs") # probs
table(yhat,sd$yr)
install.packages("dplyr", repos = c(CRAN="https://cran.r-project.org/"))
#Visualization for Legendaries and Non-Legendaries by Type
pokemon<-read.csv("Pokemon.csv",sep=",",stringsAsFactors=F)
colnames(pokemon)<-c("id","Name","Type.1","Type.2", "Total", "HP","Attack","Defense","Sp.Atk","Sp.Def","Speed","Generation","Legendary")
Type.1<-c("Dragon","Steel","Flying","Psychic","Rock" ,"Fire","Electric" ,"Dark","Ghost" ,"Ground","Ice", "Water","Grass","Fighting", "Fairy" ,"Poison","Normal","Bug")
color<-c("#6F35FC","#B7B7CE","#A98FF3","#F95587","#B6A136","#EE8130","#F7D02C","#705746","#735797","#E2BF65","#96D9D6","#6390F0","#7AC74C","#C22E28","#D685AD","#A33EA1","#A8A77A","#A6B91A")
COL<-data.frame(Type.1,color)
merge(
merge(pokemon %>% dplyr::group_by(Type.1) %>% dplyr::summarize(tot=n()),
pokemon %>% dplyr::group_by(Type.1,Legendary) %>% dplyr::summarize(count=n()),by='Type.1'),
COL, by='Type.1') %>%
ggplot(aes(x=reorder(Type.1,tot),y=count)) +
geom_bar(aes(fill=color,alpha=Legendary),color='white',size=.25,stat='identity') +
scale_fill_identity() + coord_flip() + theme_fivethirtyeight() +
ggtitle("Pokemon Distribution") + scale_alpha_discrete(range=c(.9,.6))
library(caret)
library(doSNOW)
library(magrittr)
library(dplyr)
library(ggthemes)
library(fmsb)
colnames(pokemon)<-c("id","Name","Type.1","Type.2", "Total", "HP","Attack","Defense","Sp.Atk","Sp.Def","Speed","Generation","Legendary")
Type.1<-c("Dragon","Steel","Flying","Psychic","Rock" ,"Fire","Electric" ,"Dark","Ghost" ,"Ground","Ice", "Water","Grass","Fighting", "Fairy" ,"Poison","Normal","Bug")
color<-c("#6F35FC","#B7B7CE","#A98FF3","#F95587","#B6A136","#EE8130","#F7D02C","#705746","#735797","#E2BF65","#96D9D6","#6390F0","#7AC74C","#C22E28","#D685AD","#A33EA1","#A8A77A","#A6B91A")
COL<-data.frame(Type.1,color)
merge(
merge(pokemon %>% dplyr::group_by(Type.1) %>% dplyr::summarize(tot=n()),
pokemon %>% dplyr::group_by(Type.1,Legendary) %>% dplyr::summarize(count=n()),by='Type.1'),
COL, by='Type.1') %>%
ggplot(aes(x=reorder(Type.1,tot),y=count)) +
geom_bar(aes(fill=color,alpha=Legendary),color='white',size=.25,stat='identity') +
scale_fill_identity() + coord_flip() + theme_fivethirtyeight() +
ggtitle("Pokemon Distribution") + scale_alpha_discrete(range=c(.9,.6))
#Visualization for Legendaries and Non-Legendaries by Type
pokemon<-read.csv("Pokemon.csv",sep=",",stringsAsFactors=F)
colnames(pokemon)<-c("id","Name","Type.1","Type.2", "Total", "HP","Attack","Defense","Sp.Atk","Sp.Def","Speed","Generation","Legendary")
setwd("D:/PokemonAnalytic/Classifying-Legendary-Pokemon")
#Visualization for Legendaries and Non-Legendaries by Type
pokemon<-read.csv("Pokemon.csv",sep=",",stringsAsFactors=F)
colnames(pokemon)<-c("id","Name","Type.1","Type.2", "Total", "HP","Attack","Defense","Sp.Atk","Sp.Def","Speed","Generation","Legendary")
Type.1<-c("Dragon","Steel","Flying","Psychic","Rock" ,"Fire","Electric" ,"Dark","Ghost" ,"Ground","Ice", "Water","Grass","Fighting", "Fairy" ,"Poison","Normal","Bug")
color<-c("#6F35FC","#B7B7CE","#A98FF3","#F95587","#B6A136","#EE8130","#F7D02C","#705746","#735797","#E2BF65","#96D9D6","#6390F0","#7AC74C","#C22E28","#D685AD","#A33EA1","#A8A77A","#A6B91A")
COL<-data.frame(Type.1,color)
merge(
merge(pokemon %>% dplyr::group_by(Type.1) %>% dplyr::summarize(tot=n()),
pokemon %>% dplyr::group_by(Type.1,Legendary) %>% dplyr::summarize(count=n()),by='Type.1'),
COL, by='Type.1') %>%
ggplot(aes(x=reorder(Type.1,tot),y=count)) +
geom_bar(aes(fill=color,alpha=Legendary),color='white',size=.25,stat='identity') +
scale_fill_identity() + coord_flip() + theme_fivethirtyeight() +
ggtitle("Pokemon Distribution") + scale_alpha_discrete(range=c(.9,.6))
#Loading Data
train = read.csv("Pokemon.csv", stringsAsFactors = FALSE)
View(train)
#Filling in blanks for Pokemon's second type
table(train$Type.2)
train$Type.2[train$Type.2 == ""] <- "No Second Type"
#Create Features to Distinguish Legendaries from non-Legendaries
train$OverallStrength <- ifelse(train$Total > median(train$Total),
"Strong", "Weak")
train$isDragon <- ifelse(train$Type.1 == "Dragon", "Y", "N")
train$isPsychic <- ifelse(train$Type.1 == "Psychic", "Y", "N")
train$isFlying <- ifelse(train$Type.1 == "Flying", "Y", "N")
#Factoring Categorical Data
train$Type.1 <- as.factor(train$Type.1)
train$Type.2 <- as.factor(train$Type.2)
train$Generation <- as.factor(train$Generation)
train$Legendary <- as.factor(train$Legendary)
train$OverallStrength <- as.factor(train$OverallStrength)
train$isDragon <- as.factor(train$isDragon)
train$isPsychic <- as.factor(train$isPsychic)
train$isFlying <- as.factor(train$isFlying)
View(train)
#Subsets the data we have factored and created features for
features <- c("Type.1", "Type.2", "Generation", "Legendary","OverallStrength", "isDragon", "isPsychic", "isFlying")
train <- train[, features]
str(train)
#Creating the Training and Test Splits
set.seed(54234)
indexes <- createDataPartition(train$Legendary,
times = 1,
p = 0.75,
list = FALSE)
pokemon.train <- train[indexes,]
pokemon.test <- train[-indexes,]
# Examine the proportions of the Survived class lable across
# the datasets.
prop.table(table(train$Legendary))
prop.table(table(pokemon.train$Legendary))
prop.table(table(pokemon.test$Legendary))
# Cross validation and tuning for model
train.control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3,
search = "grid")
tune.grid <- expand.grid(eta = c(0.05, 0.075, 0.1),
nrounds = c(50, 75, 100),
max_depth = 4:8,
min_child_weight = c(2.0, 2.25, 2.5),
colsample_bytree = c(0.5, 0.75, 1.0),
gamma = 0,
subsample = 1)
View(tune.grid)
#Creating Cluster
cl <- makeCluster(3, type = "SOCK")
registerDoSNOW(cl)
#Gather training model with 2 clusters to identify legendary pokemon
caret.cv <- train(Legendary ~ .,
data = pokemon.train,
method = "xgbTree",
tuneGrid = tune.grid,
trControl = train.control)
stopCluster(cl)
install.packages("e1071")
#Creating Cluster
cl <- makeCluster(3, type = "SOCK")
registerDoSNOW(cl)
#Gather training model with 2 clusters to identify legendary pokemon
caret.cv <- train(Legendary ~ .,
data = pokemon.train,
method = "xgbTree",
tuneGrid = tune.grid,
trControl = train.control)
stopCluster(cl)
caret.cv
xgbHat = predict(caret.cv, pokemon.test)
confusionMatrix(xgbHat, pokemon.test$Legendary)
#Naive Bayes Classifier
library(e1071)
NB = naiveBayes(pokemon.train, pokemon.train$Legendary,laplace = 1)
nbHat = predict(NB,pokemon.test$Legendary)
confusionMatrix(nbHat, pokemon.test$Legendary)
#KNN Classification
cl <- makeCluster(3, type = "SOCK")
registerDoSNOW(cl)
knn_fit <- train(Legendary ~., data = pokemon.train, method = "knn",
trControl= train.control,
preProcess = c("center", "scale"),
tuneLength = 10)
stopCluster(cl)
knn_fit
knnHat = predict(knn_fit, pokemon.test)
confusionMatrix(knnHat, pokemon.test$Legendary)
ctable <- as.table(confusionMatrix(knnHat, pokemon.test$Legendary), nrow = 2, byrow = TRUE)
fourfoldplot(ctable, color = c("#CC6666", "#99CC99"),
conf.level = 0, margin = 1, main = "Confusion Matrix")
library(caret)
library(doSNOW)
library(magrittr)
library(caret)
library(doSNOW)
library(magrittr)
library(dplyr)
library(ggthemes)
library(fmsb)
#Loading Data
train = read.csv("Pokemon.csv", stringsAsFactors = FALSE)
train$Type.2[train$Type.2 == ""] <- "No Second Type"
#Create Features to Distinguish Legendaries from non-Legendaries
train$OverallStrength <- ifelse(train$Total > median(train$Total),
"Strong", "Weak")
train$isDragon <- ifelse(train$Type.1 == "Dragon", "Y", "N")
train$isPsychic <- ifelse(train$Type.1 == "Psychic", "Y", "N")
train$isFlying <- ifelse(train$Type.1 == "Flying", "Y", "N")
#Factoring Categorical Data
train$Type.1 <- as.factor(train$Type.1)
train$Type.2 <- as.factor(train$Type.2)
train$Generation <- as.factor(train$Generation)
train$Legendary <- as.factor(train$Legendary)
train$OverallStrength <- as.factor(train$OverallStrength)
train$isDragon <- as.factor(train$isDragon)
train$isPsychic <- as.factor(train$isPsychic)
train$isFlying <- as.factor(train$isFlying)
#Subsets the data we have factored and created features for
features <- c("Type.1", "Type.2", "Generation", "Legendary","OverallStrength", "isDragon", "isPsychic", "isFlying")
train <- train[, features]
str(train)
#Creating the Training and Test Splits
set.seed(54234)
indexes <- createDataPartition(train$Legendary,
times = 1,
p = 0.7,
list = FALSE)
pokemon.train <- train[indexes,]
pokemon.test <- train[-indexes,]
# Examine the proportions of the Survived class lable across
# the datasets.
prop.table(table(train$Legendary))
prop.table(table(pokemon.train$Legendary))
prop.table(table(pokemon.test$Legendary))
#Creating the Training and Test Splits
set.seed(54234)
indexes <- createDataPartition(train$Legendary,
times = 1,
p = 0.65,
list = FALSE)
pokemon.train <- train[indexes,]
pokemon.test <- train[-indexes,]
# Examine the proportions of the Survived class lable across
# the datasets.
prop.table(table(train$Legendary))
prop.table(table(pokemon.train$Legendary))
prop.table(table(pokemon.test$Legendary))
#Creating the Training and Test Splits
set.seed(54234)
indexes <- createDataPartition(train$Legendary,
times = 1,
p = 0.8,
list = FALSE)
pokemon.train <- train[indexes,]
pokemon.test <- train[-indexes,]
# Examine the proportions of the Survived class lable across
# the datasets.
prop.table(table(train$Legendary))
prop.table(table(pokemon.train$Legendary))
prop.table(table(pokemon.test$Legendary))
# Cross validation and tuning for model
train.control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3,
search = "grid")
tune.grid <- expand.grid(eta = c(0.05, 0.075, 0.1),
nrounds = c(50, 75, 100),
max_depth = 4:8,
min_child_weight = c(2.0, 2.25, 2.5),
colsample_bytree = c(0.5, 0.75, 1.0),
gamma = 0,
subsample = 1)
#Creating Cluster
cl <- makeCluster(3, type = "SOCK")
registerDoSNOW(cl)
#Gather training model with 2 clusters to identify legendary pokemon
caret.cv <- train(Legendary ~ .,
data = pokemon.train,
method = "xgbTree",
tuneGrid = tune.grid,
trControl = train.control)
stopCluster(cl)
caret.cv
xgbHat = predict(caret.cv, pokemon.test)
confusionMatrix(xgbHat, pokemon.test$Legendary)
#Naive Bayes Classifier
library(e1071)
NB = naiveBayes(pokemon.train, pokemon.train$Legendary,laplace = 1)
nbHat = predict(NB,pokemon.test$Legendary)
confusionMatrix(nbHat, pokemon.test$Legendary)
#KNN Classification
cl <- makeCluster(3, type = "SOCK")
registerDoSNOW(cl)
knn_fit <- train(Legendary ~., data = pokemon.train, method = "knn",
trControl= train.control,
preProcess = c("center", "scale"),
tuneLength = 10)
stopCluster(cl)
knn_fit
knnHat = predict(knn_fit, pokemon.test)
confusionMatrix(knnHat, pokemon.test$Legendary)
tune.grid <- expand.grid(eta = c(0.05, 0.075, 0.1,0.15,0.175,0.2),
nrounds = c(50, 75, 100),
max_depth = 4:8,
min_child_weight = c(2.0, 2.25, 2.5),
colsample_bytree = c(0.5, 0.75, 1.0),
gamma = 0,
subsample = 1)
#Creating Cluster
cl <- makeCluster(3, type = "SOCK")
registerDoSNOW(cl)
#Gather training model with 2 clusters to identify legendary pokemon
caret.cv <- train(Legendary ~ .,
data = pokemon.train,
method = "xgbTree",
tuneGrid = tune.grid,
trControl = train.control)
stopCluster(cl)
caret.cv
xgbHat = predict(caret.cv, pokemon.test)
confusionMatrix(xgbHat, pokemon.test$Legendary)
# Cross validation and tuning for model
train.control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3,
search = "grid")
tune.grid <- expand.grid(eta = c(0.05, 0.075, 0.1),
nrounds = c(50, 75, 100),
max_depth = 4:8,
min_child_weight = c(2.0, 2.25, 2.5),
colsample_bytree = c(0.5, 0.75, 1.0),
gamma = 0,
subsample = 1)
#Creating Cluster
cl <- makeCluster(3, type = "SOCK")
registerDoSNOW(cl)
#Gather training model with 2 clusters to identify legendary pokemon
caret.cv <- train(Legendary ~ .,
data = pokemon.train,
method = "xgbTree",
tuneGrid = tune.grid,
trControl = train.control)
stopCluster(cl)
caret.cv
xgbHat = predict(caret.cv, pokemon.test)
confusionMatrix(xgbHat, pokemon.test$Legendary)
tune.grid <- expand.grid(eta = c( 0.05,0.1, 0.2, 0.3),
nrounds = c(50, 75, 100),
max_depth = 4:8,
min_child_weight = c(2.0, 2.25, 2.5),
colsample_bytree = c(0.5, 0.75, 1.0),
gamma = 0,
subsample = 1)
#Creating Cluster
cl <- makeCluster(3, type = "SOCK")
registerDoSNOW(cl)
#Gather training model with 2 clusters to identify legendary pokemon
caret.cv <- train(Legendary ~ .,
data = pokemon.train,
method = "xgbTree",
tuneGrid = tune.grid,
trControl = train.control)
stopCluster(cl)
caret.cv
xgbHat = predict(caret.cv, pokemon.test)
confusionMatrix(xgbHat, pokemon.test$Legendary)
tune.grid <- expand.grid(eta = c( 0.1, 0.2, 0.3, 0.4, 0.5),
nrounds = c(50, 75, 100),
max_depth = 4:8,
min_child_weight = c(2.0, 2.25, 2.5),
colsample_bytree = c(0.5, 0.75, 1.0),
gamma = 0,
subsample = 1)
#Creating Cluster
cl <- makeCluster(3, type = "SOCK")
registerDoSNOW(cl)
#Gather training model with 2 clusters to identify legendary pokemon
caret.cv <- train(Legendary ~ .,
data = pokemon.train,
method = "xgbTree",
tuneGrid = tune.grid,
trControl = train.control)
stopCluster(cl)
caret.cv
xgbHat = predict(caret.cv, pokemon.test)
confusionMatrix(xgbHat, pokemon.test$Legendary)
install.packages("tinytex")
install.packages("rmarkdown")
file.exists(Sys.which('texi2dvi'))
Sys.getenv("PATH")
Sys.setenv(PATH = paste(Sys.getenv("PATH"), "C:\Users\agrya\AppData\Local\MiKTeX\2.9\pdftex", sep=.Platform$path.sep))
Sys.setenv(PATH = paste(Sys.getenv("PATH"), "C:\\Users\agrya\AppData\Local\MiKTeX\2.9\pdftex", sep=.Platform$path.sep))
Sys.setenv(PATH = paste(Sys.getenv("PATH"), "C:/Users/agrya/AppData/Local/MiKTeX/2.9/pdftex", sep=.Platform$path.sep))
unlink('Pokemon_report_cache', recursive = TRUE)
